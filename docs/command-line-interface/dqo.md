---
title: dqo dqo command-line command
---
# dqo dqo command-line command
The reference of the **dqo** command in DQOps. DQOps Data Quality Operations Center Interactive Shell Hit @|yellow &lt;TAB&gt;|@ to see available commands. Hit @|yellow ALT-S|@ to toggle tailtips. 



___

## dqo

DQOps command-line entry point script


**Description**


*dqo* is an executable script installed in the Python scripts local folder when DQOps is installed locally by installing the *dqops* package from PyPi. When the python environment Scripts folder is in the path, running *dqo* from the command line (bash, etc.) will start a DQOps local instance.



**Command line sample**

```
$ dqo [root_level_parameter] [command]
```

**Example**

```
$ dqo --dqo.cloud.api-key=3242424324242 check run -c=connection_name
```


**Options**

| Command&nbsp;argument&nbsp;&nbsp;&nbsp;&nbsp; | Description | Required | Accepted values |
|-----------------------------------------------|-------------|:-----------------:|-----------------|
|<div id="--DQO_JAVA_OPTS">`--DQO_JAVA_OPTS`</div>|Configures additional JVM (Java Virtual Machine) options such as the memory limit. The default value for both the &#x27;dqops&#x27; python package and for the dqops/dqo Docker image is -XX:MaxRAMPercentage&#x3D;80.0 which sets the upper memory limit for 80% of the available RAM at the moment when the container starts. This parameter is not supported as a command line parameter, it is only supported as an environment variable. Set (and export) the environment variable DQO_JAVA_OPTS before starting DQOps.<br/>This parameter can also be configured by setting the *DQO_JAVA_OPTS* environment variable.| ||
|<div id="--dqo.cache.enabled">`--dqo.cache.enabled`</div>|Enables or disables the in-memory cache for parsed YAML files and Parquet data files.<br/>This parameter can also be configured by setting the *DQO_CACHE_ENABLED* environment variable.| ||
|<div id="--dqo.cache.expire-after-seconds">`--dqo.cache.expire-after-seconds`</div>|The time in seconds to expire the cache entries since they were added to the cache.<br/>This parameter can also be configured by setting the *DQO_CACHE_EXPIRE_AFTER_SECONDS* environment variable.| ||
|<div id="--dqo.cache.file-lists-limit">`--dqo.cache.file-lists-limit`</div>|The maximum number of folders for which the list of files are cached to avoid listing the files.<br/>This parameter can also be configured by setting the *DQO_CACHE_FILE_LISTS_LIMIT* environment variable.| ||
|<div id="--dqo.cache.parquet-cache-memory-fraction">`--dqo.cache.parquet-cache-memory-fraction`</div>|The maximum fraction of the JVM heap memory (configured using the -Xmx java parameter) that is used to cache parquet files in memory. The default value 0.6 means that up to 50% of the JVM heap memory can be used for caching files. The value of the reserved-heap-memory-bytes is subtracted from the total memory size (JVM&#x27;s -Xmx or -XX:MaxRAMPercentage&#x3D;80.0 parameter values) before the memory fraction is calculated. The value can be increased to 0.8 for for systems when JVM is given more than 8 GB RAM. <br/>This parameter can also be configured by setting the *DQO_CACHE_PARQUET_CACHE_MEMORY_FRACTION* environment variable.| ||
|<div id="--dqo.cache.process-file-changes-delay-millis">`--dqo.cache.process-file-changes-delay-millis`</div>|The delay in milliseconds between processing file changes that would invalidate the cache.<br/>This parameter can also be configured by setting the *DQO_CACHE_PROCESS_FILE_CHANGES_DELAY_MILLIS* environment variable.| ||
|<div id="--dqo.cache.reserved-heap-memory-bytes">`--dqo.cache.reserved-heap-memory-bytes`</div>|The memory size (in bytes) that is not subtracted from the total JVM heap memory before the memory fraction dedicated for the parquet cache is calculated. The default value is 200mb.<br/>This parameter can also be configured by setting the *DQO_CACHE_RESERVED_HEAP_MEMORY_BYTES* environment variable.| ||
|<div id="--dqo.cache.watch-file-system-changes">`--dqo.cache.watch-file-system-changes`</div>|Use a file watcher to detect file system changes and invalidate the in-memory file cache.When a file watches is enabled, all changes made to YAML files directly on the file system (i.e. by editing a file in Visual Studio Code) are instantly detected by DQOps.<br/>This parameter can also be configured by setting the *DQO_CACHE_WATCH_FILE_SYSTEM_CHANGES* environment variable.| ||
|<div id="--dqo.cache.yaml-files-limit">`--dqo.cache.yaml-files-limit`</div>|The maximum number of specification files to cache.<br/>This parameter can also be configured by setting the *DQO_CACHE_YAML_FILES_LIMIT* environment variable.| ||
|<div id="--dqo.cli.terminal.width">`--dqo.cli.terminal.width`</div>|Width of the terminal when no terminal window is available, e.g. in one-shot running mode.<br/>This parameter can also be configured by setting the *DQO_CLI_TERMINAL_WIDTH* environment variable.| ||
|<div id="--dqo.cloud.api-key">`--dqo.cloud.api-key`</div>|DQOps Cloud api key. Log in to https://cloud.dqops.com/ to get the key.<br/>This parameter can also be configured by setting the *DQO_CLOUD_API_KEY* environment variable.| ||
|<div id="--dqo.cloud.parallel-file-downloads">`--dqo.cloud.parallel-file-downloads`</div>|The number of files that are downloaded from DQOps Cloud in parallel using HTTP/2 multiplexing.<br/>This parameter can also be configured by setting the *DQO_CLOUD_PARALLEL_FILE_DOWNLOADS* environment variable.| ||
|<div id="--dqo.cloud.parallel-file-uploads">`--dqo.cloud.parallel-file-uploads`</div>|The number of files that are uploaded to DQOps Cloud in parallel using HTTP/2 multiplexing.<br/>This parameter can also be configured by setting the *DQO_CLOUD_PARALLEL_FILE_UPLOADS* environment variable.| ||
|<div id="--dqo.cloud.start-without-api-key">`--dqo.cloud.start-without-api-key`</div>|Allow starting DQOps without a DQOps Cloud API Key and without prompting to log in to DQOps Cloud.<br/>This parameter can also be configured by setting the *DQO_CLOUD_START_WITHOUT_API_KEY* environment variable.| ||
|<div id="--dqo.core.lock-wait-timeout-seconds">`--dqo.core.lock-wait-timeout-seconds`</div>|Sets the maximum wait timeout in seconds to obtain a lock to read or write files.<br/>This parameter can also be configured by setting the *DQO_CORE_LOCK_WAIT_TIMEOUT_SECONDS* environment variable.| ||
|<div id="--dqo.core.print-stack-trace">`--dqo.core.print-stack-trace`</div>|Prints a full stack trace for errors on the console.<br/>This parameter can also be configured by setting the *DQO_CORE_PRINT_STACK_TRACE* environment variable.| ||
|<div id="--dqo.default-time-zone">`--dqo.default-time-zone`</div>|Default time zone name used to convert the server&#x27;s local dates to a local time in a time zone that is relevant for the user. Use official IANA time zone names. When the parameter is not configured, DQOps uses the local time zone of the host running the application. The time zone can be reconfigured at a user settings level.<br/>This parameter can also be configured by setting the *DQO_DEFAULT_TIME_ZONE* environment variable.| ||
|<div id="--dqo.docker.user-home.allow-unmounted">`--dqo.docker.user-home.allow-unmounted`</div>|When running DQOps in a docker container, allow DQOps user home folder to be initialized inside the container&#x27;s filesystem if the folder hasn&#x27;t been mounted to an external volume.<br/>This parameter can also be configured by setting the *DQO_DOCKER_USER_HOME_ALLOW_UNMOUNTED* environment variable.| ||
|<div id="--dqo.duckdb.memory-limit">`--dqo.duckdb.memory-limit`</div>|The maximum memory of the system (e.g., 1GB). When not set, DuckDB use the 80% of RAM.<br/>This parameter can also be configured by setting the *DQO_DUCKDB_MEMORY_LIMIT* environment variable.| ||
|<div id="--dqo.duckdb.threads">`--dqo.duckdb.threads`</div>|The number of total threads used by the system. The default value is 1000<br/>This parameter can also be configured by setting the *DQO_DUCKDB_THREADS* environment variable.| ||
|<div id="--dqo.error-sampling.samples-limit">`--dqo.error-sampling.samples-limit`</div>|The maximum number of error samples (invalid column values) captured for each data grouping when data grouping is configured, or for the whole table when data grouping is not configured.<br/>This parameter can also be configured by setting the *DQO_ERROR_SAMPLING_SAMPLES_LIMIT* environment variable.| ||
|<div id="--dqo.error-sampling.total-samples-limit">`--dqo.error-sampling.total-samples-limit`</div>|The maximum total number of error sampling results captured from a table when data grouping is enabled on a table, and error samples are captured from multiple data groupings..<br/>This parameter can also be configured by setting the *DQO_ERROR_SAMPLING_TOTAL_SAMPLES_LIMIT* environment variable.| ||
|<div id="--dqo.error-sampling.truncated-strings-length">`--dqo.error-sampling.truncated-strings-length`</div>|The maximum length of error samples captured from text columns (varchar, string, text, etc.) that are stored as error samples table. DQOps truncates longer column values and stores only the first few characters, up to the character count limit defined by this parameter.<br/>This parameter can also be configured by setting the *DQO_ERROR_SAMPLING_TRUNCATED_STRINGS_LENGTH* environment variable.| ||
|<div id="--dqo.home">`--dqo.home`</div>|Overrides the path to the DQOps system home (DQO_HOME). The default DQO_HOME contains the definition of built-in data quality sensors, rules and libraries.<br/>This parameter can also be configured by setting the *DQO_HOME* environment variable.| ||
|<div id="--dqo.incidents.check-histogram-size">`--dqo.incidents.check-histogram-size`</div>|The size of the data quality check histogram that is generated for a preview of a data quality incident.<br/>This parameter can also be configured by setting the *DQO_INCIDENTS_CHECK_HISTOGRAM_SIZE* environment variable.| ||
|<div id="--dqo.incidents.column-histogram-size">`--dqo.incidents.column-histogram-size`</div>|The size of the column histogram that is generated for a preview of a data quality incident.<br/>This parameter can also be configured by setting the *DQO_INCIDENTS_COLUMN_HISTOGRAM_SIZE* environment variable.| ||
|<div id="--dqo.incidents.count-open-incidents-days">`--dqo.incidents.count-open-incidents-days`</div>|The time window between now and X days ago to scan for open incidents that are shown on the list of connections in the incidents section.<br/>This parameter can also be configured by setting the *DQO_INCIDENTS_COUNT_OPEN_INCIDENTS_DAYS* environment variable.| ||
|<div id="--dqo.incidents.partitioned-checks-time-window-days">`--dqo.incidents.partitioned-checks-time-window-days`</div>|The time window for the maximum age of a daily or monthly partition whose data quality issues are included in new data quality incidents when an issue is detected. Data quality issues on older partitions will not trigger creating a new incident.<br/>This parameter can also be configured by setting the *DQO_INCIDENTS_PARTITIONED_CHECKS_TIME_WINDOW_DAYS* environment variable.| ||
|<div id="--dqo.incidents.top-incidents-days">`--dqo.incidents.top-incidents-days`</div>|The time window between now and X days ago to scan for incidents that are shown on the main screen, grouped by a requested grouping.<br/>This parameter can also be configured by setting the *DQO_INCIDENTS_TOP_INCIDENTS_DAYS* environment variable.| ||
|<div id="--dqo.instance.name">`--dqo.instance.name`</div>|DQOps instance name. DQOps uses this instance name when finding which data quality checks should be run on this DQOps instance. When a connection is limited to run scheduled data quality checks only on a named instance, the instance name must math. This parameter can be overwritten in the instance&#x27;s local settings file.<br/>This parameter can also be configured by setting the *DQO_INSTANCE_NAME* environment variable.| ||
|<div id="--dqo.instance.return-base-url">`--dqo.instance.return-base-url`</div>|Base url of this instance that is used as a return url when authentication with DQOps Cloud credentials is forwarded and the user must be forwarded back to the current instance from the https://cloud.dqops.com login screen. When this parameter is not provided, DQOps will use the url from the &quot;Host&quot; HTTP header.<br/>This parameter can also be configured by setting the *DQO_INSTANCE_RETURN_BASE_URL* environment variable.| ||
|<div id="--dqo.instance.signature-key">`--dqo.instance.signature-key`</div>|DQOps local instance signature key that is used to issue and verify digital signatures on API keys. It is a base64 encoded byte array (32 bytes). When not configured, DQOps will generate a secure random key and store it in the .localsettings.dqosettings.yaml file.<br/>This parameter can also be configured by setting the *DQO_INSTANCE_SIGNATURE_KEY* environment variable.| ||
|<div id="--dqo.integrations.table-health-webhook-urls">`--dqo.integrations.table-health-webhook-urls`</div>|A comma separated list of webhook URLs where DQOps sends updates of the table data quality status changes.<br/>This parameter can also be configured by setting the *DQO_INTEGRATIONS_TABLE_HEALTH_WEBHOOK_URLS* environment variable.| ||
|<div id="--dqo.jdbc.expire-after-access-seconds">`--dqo.jdbc.expire-after-access-seconds`</div>|Sets the number of seconds when a connection in a JDBC pool is expired after the last access.<br/>This parameter can also be configured by setting the *DQO_JDBC_EXPIRE_AFTER_ACCESS_SECONDS* environment variable.| ||
|<div id="--dqo.jdbc.max-connection-in-pool">`--dqo.jdbc.max-connection-in-pool`</div>|Sets the maximum number of connections in the JDBC connection pool, shared across all data sources using JDBC drivers.<br/>This parameter can also be configured by setting the *DQO_JDBC_MAX_CONNECTION_IN_POOL* environment variable.| ||
|<div id="--dqo.logging.console">`--dqo.logging.console`</div>|Enables logging to console, selecting the correct format. The default configuration &#x27;OFF&#x27; disables console logging, allowing to use the DQOps shell without being distracted by log entries. Set the &#x27;PATTERN&#x27; mode to send formatted entries to the console in a format similar to Apache logs. When running DQOps in as a docker container on a Kubernetes engine that is configured to capture DQOps container logs, use &#x27;JSON&#x27; mode to publish structured Json log entries that can be parsed by fluentd or other similar log engines. JSON formatted messages use a Logstash compatible format.<br/>This parameter can also be configured by setting the *DQO_LOGGING_CONSOLE* environment variable.| |*OFF*<br/>*JSON*<br/>*PATTERN*<br/>|
|<div id="--dqo.logging.console-immediate-flush">`--dqo.logging.console-immediate-flush`</div>|When the console logging is enabled with --dqo.logging.console&#x3D;PATTERN or --dqo.logging.console&#x3D;JSON, turns on (for &#x27;true&#x27;) or turns of (for &#x27;false&#x27;) immediate console flushing after each log entry was written. Immediate console flushing is desirable when DQOps is started as a docker container and docker logs from DQOps should be forwarded to Kubernetes for centralized logging.<br/>This parameter can also be configured by setting the *DQO_LOGGING_CONSOLE_IMMEDIATE_FLUSH* environment variable.| ||
|<div id="--dqo.logging.enable-user-home-logging">`--dqo.logging.enable-user-home-logging`</div>|Enables file logging inside the DQOps User Home&#x27;s .logs folder.<br/>This parameter can also be configured by setting the *DQO_LOGGING_ENABLE_USER_HOME_LOGGING* environment variable.| ||
|<div id="--dqo.logging.max-history">`--dqo.logging.max-history`</div>|Sets the maximum number of log files that can be stored (archived) in the .logs folder.<br/>This parameter can also be configured by setting the *DQO_LOGGING_MAX_HISTORY* environment variable.| ||
|<div id="--dqo.logging.pattern">`--dqo.logging.pattern`</div>|Log entry pattern for logback used for writing log entries.<br/>This parameter can also be configured by setting the *DQO_LOGGING_PATTERN* environment variable.| ||
|<div id="--dqo.logging.total-size-cap">`--dqo.logging.total-size-cap`</div>|Total log file size cap of log files generated in the DQOps User Home&#x27;s .logs folder. Supported suffixes are: kb, mb, gb. For example: 10mb, 2gb.<br/>This parameter can also be configured by setting the *DQO_LOGGING_TOTAL_SIZE_CAP* environment variable.| ||
|<div id="--dqo.logging.user-errors.checks-log-level">`--dqo.logging.user-errors.checks-log-level`</div>|The logging level at which any errors captured during the data quality check evaluation are reported. When a data quality check is executed and the error is related to a sensor (query) or a rule (python) function, they are reported as sensor or rules issues.Only data quality check configuration issues that prevent running a data quality check are reported as check issues. The logging level for the whole application must be equal or higher to this level for effective logging. Check logs are logged under the com.dqops.user-errors.checks log.<br/>This parameter can also be configured by setting the *DQO_LOGGING_USER_ERRORS_CHECKS_LOG_LEVEL* environment variable.| |*ERROR*<br/>*WARN*<br/>*INFO*<br/>*DEBUG*<br/>*TRACE*<br/>|
|<div id="--dqo.logging.user-errors.rules-log-level">`--dqo.logging.user-errors.rules-log-level`</div>|The logging level at which any errors captured during the data quality rule (python function) evaluation are reported. The logging level for the whole application must be equal or higher to this level for effective logging. Rule logs are logged under the com.dqops.user-errors.rules log.<br/>This parameter can also be configured by setting the *DQO_LOGGING_USER_ERRORS_RULES_LOG_LEVEL* environment variable.| |*ERROR*<br/>*WARN*<br/>*INFO*<br/>*DEBUG*<br/>*TRACE*<br/>|
|<div id="--dqo.logging.user-errors.sensors-log-level">`--dqo.logging.user-errors.sensors-log-level`</div>|The logging level at which any errors captured during the data quality sensor (query) execution are reported. The logging level for the whole application must be equal or higher to this level for effective logging. Sensor logs are logged under the com.dqops.user-errors.sensors log.<br/>This parameter can also be configured by setting the *DQO_LOGGING_USER_ERRORS_SENSORS_LOG_LEVEL* environment variable.| |*ERROR*<br/>*WARN*<br/>*INFO*<br/>*DEBUG*<br/>*TRACE*<br/>|
|<div id="--dqo.logging.user-errors.statistics-log-level">`--dqo.logging.user-errors.statistics-log-level`</div>|The logging level at which any errors captured during the statistics collection are reported. The logging level for the whole application must be equal or higher to this level for effective logging. Statistics logs are logged under the com.dqops.user-errors.statistics log.<br/>This parameter can also be configured by setting the *DQO_LOGGING_USER_ERRORS_STATISTICS_LOG_LEVEL* environment variable.| |*ERROR*<br/>*WARN*<br/>*INFO*<br/>*DEBUG*<br/>*TRACE*<br/>|
|<div id="--dqo.logging.user-errors.yaml-log-level">`--dqo.logging.user-errors.yaml-log-level`</div>|The logging level at which any errors captured during YAML file parsing are reported. The logging level for the whole application must be equal or higher to this level for effective logging. Statistics logs are logged under the com.dqops.user-errors.yaml log.<br/>This parameter can also be configured by setting the *DQO_LOGGING_USER_ERRORS_YAML_LOG_LEVEL* environment variable.| |*ERROR*<br/>*WARN*<br/>*INFO*<br/>*DEBUG*<br/>*TRACE*<br/>|
|<div id="--dqo.metadata.auto-import-tables-limit">`--dqo.metadata.auto-import-tables-limit`</div>|Sets the maximum number of tables that are imported from a data source by the auto import that is scheduled on the DQOps CRON scheduler.<br/>This parameter can also be configured by setting the *DQO_METADATA_AUTO_IMPORT_TABLES_LIMIT* environment variable.| ||
|<div id="--dqo.metadata.import.tables-import-limit">`--dqo.metadata.import.tables-import-limit`</div>|Sets the maximum number of tables that are imported from a data source. DQOps supports importing more tables by importing additional tables specified by a different table filter.<br/>This parameter can also be configured by setting the *DQO_METADATA_IMPORT_TABLES_IMPORT_LIMIT* environment variable.| ||
|<div id="--dqo.python.interpreter-name">`--dqo.python.interpreter-name`</div>|A list of python interpreter executable names, separated by a comma, containing possible python interpreter names such as &#x27;python&#x27;, &#x27;python3&#x27;, &#x27;python3.exe&#x27; or an absolute path to the python interpreter. DQOps will try to find the first python interpreter executable in directories defined in the PATH when a list of python interpreter names (not an absolute path) is used.<br/>This parameter can also be configured by setting the *DQO_PYTHON_INTERPRETER_NAME* environment variable.| ||
|<div id="--dqo.python.python-script-timeout-seconds">`--dqo.python.python-script-timeout-seconds`</div>|Python script execution time limit in seconds for running jinja2 and rule evaluation scripts.<br/>This parameter can also be configured by setting the *DQO_PYTHON_PYTHON_SCRIPT_TIMEOUT_SECONDS* environment variable.| ||
|<div id="--dqo.python.use-host-python">`--dqo.python.use-host-python`</div>|Disable creating a python virtual environment by DQOps on startup. Instead, use the system python interpreter. DQOps will not install any required python packages on startup and use packages from the user&#x27;s python installation.<br/>This parameter can also be configured by setting the *DQO_PYTHON_USE_HOST_PYTHON* environment variable.| ||
|<div id="--dqo.queue.max-concurrent-jobs">`--dqo.queue.max-concurrent-jobs`</div>|Sets the maximum number of concurrent jobs that the job queue can process at once (running data quality checks, importing metadata, etc.). The maximum number of threads is also limited by the DQOps license.<br/>This parameter can also be configured by setting the *DQO_QUEUE_MAX_CONCURRENT_JOBS* environment variable.| ||
|<div id="--dqo.queue.wait-timeouts.collect-statistics">`--dqo.queue.wait-timeouts.collect-statistics`</div>|Sets the default timeout (in seconds) for the &quot;collect statistics&quot; REST API operation called from the DQOps client when the &quot;wait&quot; parameter is true and the timeout is not provided by the client.<br/>This parameter can also be configured by setting the *DQO_QUEUE_WAIT_TIMEOUTS_COLLECT_STATISTICS* environment variable.| ||
|<div id="--dqo.queue.wait-timeouts.default-wait-timeout">`--dqo.queue.wait-timeouts.default-wait-timeout`</div>|Sets the default wait timeout (in seconds) for waiting for a job when the &quot;waitTimeout&quot; parameter is not given to the call to the &quot;waitForJob&quot; operation from the DQOps client..<br/>This parameter can also be configured by setting the *DQO_QUEUE_WAIT_TIMEOUTS_DEFAULT_WAIT_TIMEOUT* environment variable.| ||
|<div id="--dqo.queue.wait-timeouts.delete-stored-data">`--dqo.queue.wait-timeouts.delete-stored-data`</div>|Sets the default timeout (in seconds) for the &quot;delete stored data&quot; rest api operation called from the DQOps client when the &quot;wait&quot; parameter is true and the timeout is not provided by the client.<br/>This parameter can also be configured by setting the *DQO_QUEUE_WAIT_TIMEOUTS_DELETE_STORED_DATA* environment variable.| ||
|<div id="--dqo.queue.wait-timeouts.import-tables">`--dqo.queue.wait-timeouts.import-tables`</div>|Sets the default timeout (in seconds) for the &quot;import tables&quot; rest api operation called from the DQOps client when the &quot;wait&quot; parameter is true and the timeout is not provided by the client.<br/>This parameter can also be configured by setting the *DQO_QUEUE_WAIT_TIMEOUTS_IMPORT_TABLES* environment variable.| ||
|<div id="--dqo.queue.wait-timeouts.run-checks">`--dqo.queue.wait-timeouts.run-checks`</div>|Sets the default timeout (in seconds) for the &quot;run checks&quot; rest api operation called from the DQOps client when the &quot;wait&quot; parameter is true and the timeout is not provided by the client.<br/>This parameter can also be configured by setting the *DQO_QUEUE_WAIT_TIMEOUTS_RUN_CHECKS* environment variable.| ||
|<div id="--dqo.queue.wait-timeouts.synchronize-multiple-folders">`--dqo.queue.wait-timeouts.synchronize-multiple-folders`</div>|Sets the default timeout (in seconds) for the &quot;synchronize multiple folders&quot; rest api operation called from the DQOps client when the &quot;wait&quot; parameter is true and the timeout is not provided by the client.<br/>This parameter can also be configured by setting the *DQO_QUEUE_WAIT_TIMEOUTS_SYNCHRONIZE_MULTIPLE_FOLDERS* environment variable.| ||
|<div id="--dqo.rule-mining.days-in-range-max-date-days-ahead">`--dqo.rule-mining.days-in-range-max-date-days-ahead`</div>|The number of days to add to the current system date that DQOps rule mining engine uses to set the maximum date in the date_in_range data quality check. The default configuration sets a date that is 10 years ahead.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_DAYS_IN_RANGE_MAX_DATE_DAYS_AHEAD* environment variable.| ||
|<div id="--dqo.rule-mining.days-in-range-min-date-days-before">`--dqo.rule-mining.days-in-range-min-date-days-before`</div>|The number of days to subtract from the earliest found date in a column that DQOps rule mining engine uses to set the minimum date in the date_in_range data quality check. The default configuration sets a date that is 2 days before.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_DAYS_IN_RANGE_MIN_DATE_DAYS_BEFORE* environment variable.| ||
|<div id="--dqo.rule-mining.default-fail-checks-at-percent-error-rows">`--dqo.rule-mining.default-fail-checks-at-percent-error-rows`</div>|The default percentage value captured by a profiling check (for example 0.03% of errors or 99.97% of valid) that is used to propose a percentage rule that will treat the values as errors (i.e., max_percent &#x3D; 0%, or min_percent &#x3D; 100%).The default value is 2%.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_DEFAULT_FAIL_CHECKS_AT_PERCENT_ERROR_ROWS* environment variable.| ||
|<div id="--dqo.rule-mining.default-max-percent-error-rows">`--dqo.rule-mining.default-max-percent-error-rows`</div>|The default maximum percentage of invalid rows for which the rule engine should configure rule values, especially min_percent, min_count or max_percent.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_DEFAULT_MAX_PERCENT_ERROR_ROWS* environment variable.| ||
|<div id="--dqo.rule-mining.max-column-samples-to-propose-accepted-values">`--dqo.rule-mining.max-column-samples-to-propose-accepted-values`</div>|The maximum number of samples in the sample values to use when generating a proposed configuration of the accepted_values checks. If a column has more than this number of distinct values, DQOps will not configure the found in set checks. The default value is 50 samples.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_MAX_COLUMN_SAMPLES_TO_PROPOSE_ACCEPTED_VALUES* environment variable.| ||
|<div id="--dqo.rule-mining.max-distinct-count">`--dqo.rule-mining.max-distinct-count`</div>|The default maximum distinct count that is used to activate the distinct_count check. Above the limit, DQOps will configure the distinct_percent check. The default value is 1000 rows.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_MAX_DISTINCT_COUNT* environment variable.| ||
|<div id="--dqo.rule-mining.max-expected-texts-in-top-values">`--dqo.rule-mining.max-expected-texts-in-top-values`</div>|The maximum number of top (most common) text values that are added to the expected_texts_in_top_values check. The default value is 5 top text values.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_MAX_EXPECTED_TEXTS_IN_TOP_VALUES* environment variable.| ||
|<div id="--dqo.rule-mining.min-count-rate">`--dqo.rule-mining.min-count-rate`</div>|The rate of the current min_count value, such as the row count, or the count of not-null values, obtained from statistics that is used to configure the row_count and not_nulls_count checks by the check mining engine. The default value is 0.90 of the current count captured by the statistics (row count, not-nulls count).<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_MIN_COUNT_RATE* environment variable.| ||
|<div id="--dqo.rule-mining.min-max-value-rate-delta">`--dqo.rule-mining.min-max-value-rate-delta`</div>|The default delta that is added to the proposed maximum value, or subtracted from a proposed minimum value. It is calculated as a rate of the current value. The default value is 0.1, which means that when the detected minimum value is 5.0, the proposed minimum value in the rule will be 4.5.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_MIN_MAX_VALUE_RATE_DELTA* environment variable.| ||
|<div id="--dqo.rule-mining.min-reasonable-not-null-count">`--dqo.rule-mining.min-reasonable-not-null-count`</div>|The default minimum reasonable count of not-null values that must be satisfied to apply some checks that validate a range of row counts (like a distinct count between).<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_MIN_REASONABLE_NOT_NULL_COUNT* environment variable.| ||
|<div id="--dqo.rule-mining.min-word-count">`--dqo.rule-mining.min-word-count`</div>|The minimum word count that is required to apply word count count in range checks. The default value is 3 words.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_MIN_WORD_COUNT* environment variable.| ||
|<div id="--dqo.rule-mining.not-null-count-rate-for-duplicate-count">`--dqo.rule-mining.not-null-count-rate-for-duplicate-count`</div>|The default rate (fraction) of the number of rows with not-null values that must contain distinct values to apply the distinct count between check. The default is 0.01, which is 1% of the count of not-null values.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_NOT_NULL_COUNT_RATE_FOR_DUPLICATE_COUNT* environment variable.| ||
|<div id="--dqo.rule-mining.percent-check-delta-rate">`--dqo.rule-mining.percent-check-delta-rate`</div>|The multiplier of the last known percent that is extended by this delta (as a rate/proportion of the percentage) to configure a passing percentage check.The default value is 0.3. For this value and when the last known max_percent was 10%, DQOps rule mining engine will propose a save max_count 13%. The additional 3% of the delta is 0.3 * 10%.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_PERCENT_CHECK_DELTA_RATE* environment variable.| ||
|<div id="--dqo.rule-mining.timeliness-max-days-multiplier">`--dqo.rule-mining.timeliness-max-days-multiplier`</div>|he multiplier of the last known table timeliness checks (freshness, staleness, ingestion delay) that is used to propose the configuration of the max days rule threshold by the rule mining engine.The default value is 2.0x the last known delay.<br/>This parameter can also be configured by setting the *DQO_RULE_MINING_TIMELINESS_MAX_DAYS_MULTIPLIER* environment variable.| ||
|<div id="--dqo.scheduler.check-run-mode">`--dqo.scheduler.check-run-mode`</div>|Configures the console logging mode for the &#x27;&quot;check run&quot; jobs performed by the job scheduler in the background.<br/>This parameter can also be configured by setting the *DQO_SCHEDULER_CHECK_RUN_MODE* environment variable.| |*silent*<br/>*summary*<br/>*info*<br/>*debug*<br/>|
|<div id="--dqo.scheduler.default-schedules.monitoring-daily">`--dqo.scheduler.default-schedules.monitoring-daily`</div>|Sets the default schedule for running daily monitoring checks that is copied to the configuration of new data source connections that are registered in DQOps. The default schedule runs checks once a day at 12 PM (noon). This parameter is used only once, during the first initialization of DQOps user home. The value is copied to the settings/defaultschedules.dqoschedules.yaml file.<br/>This parameter can also be configured by setting the *DQO_SCHEDULER_DEFAULT_SCHEDULES_MONITORING_DAILY* environment variable.| ||
|<div id="--dqo.scheduler.default-schedules.monitoring-monthly">`--dqo.scheduler.default-schedules.monitoring-monthly`</div>|Sets the default schedule for running monthly monitoring checks that is copied to the configuration of new data source connections that are registered in DQOps. The default schedule runs checks once a day at 12 PM (noon). This parameter is used only once, during the first initialization of DQOps user home. The value is copied to the settings/defaultschedules.dqoschedules.yaml file.<br/>This parameter can also be configured by setting the *DQO_SCHEDULER_DEFAULT_SCHEDULES_MONITORING_MONTHLY* environment variable.| ||
|<div id="--dqo.scheduler.default-schedules.partitioned-daily">`--dqo.scheduler.default-schedules.partitioned-daily`</div>|Sets the default schedule for running daily partitioned checks that is copied to the configuration of new data source connections that are registered in DQOps. The default schedule runs checks once a day at 12 PM (noon). This parameter is used only once, during the first initialization of DQOps user home. The value is copied to the settings/defaultschedules.dqoschedules.yaml file.<br/>This parameter can also be configured by setting the *DQO_SCHEDULER_DEFAULT_SCHEDULES_PARTITIONED_DAILY* environment variable.| ||
|<div id="--dqo.scheduler.default-schedules.partitioned-monthly">`--dqo.scheduler.default-schedules.partitioned-monthly`</div>|Sets the default schedule for running monthly partitioned checks that is copied to the configuration of new data source connections that are registered in DQOps. The default schedule runs checks once a day at 12 PM (noon). This parameter is used only once, during the first initialization of DQOps user home. The value is copied to the .settings/defaultschedules.dqoschedules.yaml file.<br/>This parameter can also be configured by setting the *DQO_SCHEDULER_DEFAULT_SCHEDULES_PARTITIONED_MONTHLY* environment variable.| ||
|<div id="--dqo.scheduler.default-schedules.profiling">`--dqo.scheduler.default-schedules.profiling`</div>|Sets the default schedule for running profiling checks that is copied to the configuration of new data source connections that are registered in DQOps. The default schedule runs profiling checks once a month, on the first day of the month at 1 AM. This parameter is used only once, during the first initialization of DQOps user home. The value is copied to the settings/defaultschedules.dqoschedules.yaml file.<br/>This parameter can also be configured by setting the *DQO_SCHEDULER_DEFAULT_SCHEDULES_PROFILING* environment variable.| ||
|<div id="--dqo.scheduler.enable-cloud-sync">`--dqo.scheduler.enable-cloud-sync`</div>|Enable synchronization of metadata and results with DQOps Cloud in the job scheduler.<br/>This parameter can also be configured by setting the *DQO_SCHEDULER_ENABLE_CLOUD_SYNC* environment variable.| ||
|<div id="--dqo.scheduler.start">`--dqo.scheduler.start`</div>|Starts the job scheduler on startup (true) or disables the job scheduler (false).<br/>This parameter can also be configured by setting the *DQO_SCHEDULER_START* environment variable.| ||
|<div id="--dqo.scheduler.synchronization-mode">`--dqo.scheduler.synchronization-mode`</div>|Configures the console logging mode for the &#x27;&quot;cloud sync all&quot; operations performed by the job scheduler in the background.<br/>This parameter can also be configured by setting the *DQO_SCHEDULER_SYNCHRONIZATION_MODE* environment variable.| |*silent*<br/>*summary*<br/>*debug*<br/>|
|<div id="--dqo.scheduler.synchronize-cron-schedule">`--dqo.scheduler.synchronize-cron-schedule`</div>|Unix cron expression to configure how often the scheduler will synchronize the local copy of the metadata with DQOps Cloud and detect new cron schedules. The default schedule will synchronize local files with DQOps Cloud and refresh the data quality data warehouse 5 minutes past each hour. A DQOps instance that uses a FREE or a trial PERSONAL license will ignore this setting and synchronize files once an hour, on a random time. Synchronization with DQOps cloud can be disabled by setting --dqo.scheduler.enable-cloud-sync&#x3D;false.<br/>This parameter can also be configured by setting the *DQO_SCHEDULER_SYNCHRONIZE_CRON_SCHEDULE* environment variable.| ||
|<div id="--dqo.scheduler.synchronized-folders">`--dqo.scheduler.synchronized-folders`</div>|Configures which folders from the DQOps user home folder are synchronized to DQOps Cloud during a monitoring synchronization (triggered by a cron schedule configured by --dqo.scheduler.synchronize-cron-schedule). By default, DQOps synchronizes (pushes) only changes from folders that have local changes.<br/>This parameter can also be configured by setting the *DQO_SCHEDULER_SYNCHRONIZED_FOLDERS* environment variable.| |*all*<br/>*locally_changed*<br/>|
|<div id="--dqo.secrets.enable-gcp-secret-manager">`--dqo.secrets.enable-gcp-secret-manager`</div>|Enables GCP secret manager to resolve parameters like null in the yaml files.<br/>This parameter can also be configured by setting the *DQO_SECRETS_ENABLE_GCP_SECRET_MANAGER* environment variable.| ||
|<div id="--dqo.secrets.gcp-project-id">`--dqo.secrets.gcp-project-id`</div>|GCP project name with a GCP secret manager enabled to pull the secrets.<br/>This parameter can also be configured by setting the *DQO_SECRETS_GCP_PROJECT_ID* environment variable.| ||
|<div id="--dqo.sensor.limit.fail-on-sensor-readout-limit-exceeded">`--dqo.sensor.limit.fail-on-sensor-readout-limit-exceeded`</div>|Configures the behavior when the number of rows returned from a data quality sensor exceeds the limit configured in the &#x27;sensor-readout-limit&#x27; parameter. When true, the whole check execution is failed. When false, only results up to the limit are analyzed. The default value is true.<br/>This parameter can also be configured by setting the *DQO_SENSOR_LIMIT_FAIL_ON_SENSOR_READOUT_LIMIT_EXCEEDED* environment variable.| ||
|<div id="--dqo.sensor.limit.max-merged-queries">`--dqo.sensor.limit.max-merged-queries`</div>|The maximum number of queries that are merged into a bigger query, to calculate multiple sensors on the same table and to analyze multiple columns from the same table.<br/>This parameter can also be configured by setting the *DQO_SENSOR_LIMIT_MAX_MERGED_QUERIES* environment variable.| ||
|<div id="--dqo.sensor.limit.sensor-readout-limit">`--dqo.sensor.limit.sensor-readout-limit`</div>|Default row count limit retrieved by a data quality sensor from the results of an SQL query for non-partitioned checks (profiling and monitoring). This is the row count limit applied when querying the data source. When the data grouping configuration sets up a GROUP BY too many columns or columns with too many distinct values, the data source will return too many results to store them as data quality check results and sensor readouts. DQOps will discard additional values returned from the data source or raise an error.<br/>This parameter can also be configured by setting the *DQO_SENSOR_LIMIT_SENSOR_READOUT_LIMIT* environment variable.| ||
|<div id="--dqo.sensor.limit.sensor-readout-limit-partitioned">`--dqo.sensor.limit.sensor-readout-limit-partitioned`</div>|Default row count limit retrieved by a data quality sensor from the results of an SQL query for partitioned checks. This is the row count limit applied when querying the data source. When the data grouping configuration sets up a GROUP BY too many columns or columns with too many distinct values, the data source will return too many results to store them as data quality check results and sensor readouts. DQOps will discard additional values returned from the data source or return an error. The default value is 7x bigger than the sensor-readout-limit to allow analysing the last 7 daily partitions.<br/>This parameter can also be configured by setting the *DQO_SENSOR_LIMIT_SENSOR_READOUT_LIMIT_PARTITIONED* environment variable.| ||
|<div id="--dqo.smtp-server.host">`--dqo.smtp-server.host`</div>|Sets the  host name of the SMTP server that is used to send email notifications.<br/>This parameter can also be configured by setting the *DQO_SMTP_SERVER_HOST* environment variable.| ||
|<div id="--dqo.smtp-server.password">`--dqo.smtp-server.password`</div>|Sets the password of the SMTP server that is used to send email notifications.<br/>This parameter can also be configured by setting the *DQO_SMTP_SERVER_PASSWORD* environment variable.| ||
|<div id="--dqo.smtp-server.port">`--dqo.smtp-server.port`</div>|Sets the port number of the SMTP server that is used to send email notifications.<br/>This parameter can also be configured by setting the *DQO_SMTP_SERVER_PORT* environment variable.| ||
|<div id="--dqo.smtp-server.use-ssl">`--dqo.smtp-server.use-ssl`</div>|Configures if the SMTP server that is used to send email notifications uses SSL protocol.<br/>This parameter can also be configured by setting the *DQO_SMTP_SERVER_USE_SSL* environment variable.| ||
|<div id="--dqo.smtp-server.username">`--dqo.smtp-server.username`</div>|Sets the username of the SMTP server that is used to send email notifications.<br/>This parameter can also be configured by setting the *DQO_SMTP_SERVER_USERNAME* environment variable.| ||
|<div id="--dqo.statistics.samples-limit">`--dqo.statistics.samples-limit`</div>|The limit of column value samples that are collected when the basic table statistics are gathered. DQOps collects only the most popular values, which is determined by the number of value occurrences.<br/>This parameter can also be configured by setting the *DQO_STATISTICS_SAMPLES_LIMIT* environment variable.| ||
|<div id="--dqo.statistics.truncated-strings-length">`--dqo.statistics.truncated-strings-length`</div>|The length of samples captured from text columns (varchar, string, text, etc.) that are stored as samples. DQOps truncates longer column values and stores only the first few characters, up to the character count limit defined by this parameter.<br/>This parameter can also be configured by setting the *DQO_STATISTICS_TRUNCATED_STRINGS_LENGTH* environment variable.| ||
|<div id="--dqo.statistics.viewed-statistics-age-months">`--dqo.statistics.viewed-statistics-age-months`</div>|The maximum age (in months) of the basic statistics that are shown on the basic statistics screen. Statistics values captured earlier are still stored, but are not shown in the DQOps UI.<br/>This parameter can also be configured by setting the *DQO_STATISTICS_VIEWED_STATISTICS_AGE_MONTHS* environment variable.| ||
|<div id="--dqo.user.home">`--dqo.user.home`</div>|Overrides the path to the DQOps user home. The default user home is created in the current folder (.).<br/>This parameter can also be configured by setting the *DQO_USER_HOME* environment variable.| ||
|<div id="--dqo.user.initialize-user-home">`--dqo.user.initialize-user-home`</div>|Initializes an empty DQOps user home (identified by the DQO_USER_HOME environment variable) without asking the user for confirmation.<br/>This parameter can also be configured by setting the *DQO_USER_INITIALIZE_USER_HOME* environment variable.| ||
|<div id="--dqo.webserver.authentication-method">`--dqo.webserver.authentication-method`</div>|User authentication method. A standalone instance has no user authentication. Paid versions of DQOps support federated authentication using Single-Sign-On. Please contact DQOps sales for details: https://dqops.com/contact-us/.<br/>This parameter can also be configured by setting the *DQO_WEBSERVER_AUTHENTICATION_METHOD* environment variable.| |*none*<br/>*dqops_cloud*<br/>*oauth2*<br/>|
|<div id="-fw">`-fw`</div><div id="--file-write">`--file-write`</div>|Write command response to a file<br/>This parameter can also be configured by setting the *_FW**FILE_WRITE* environment variable.| ||
|<div id="--headless">`--headless`</div><div id="-hl">`-hl`</div>|Starts DQOps in a headless mode. When DQOps runs in a headless mode and the application cannot start because the DQOps Cloud API key is missing or the DQOps user home folder is not configured, DQOps will stop silently instead of asking the user to approve the setup of the DQOps user home folder structure and/or log into DQOps Cloud.<br/>This parameter can also be configured by setting the *HEADLESS**_HL* environment variable.| ||
|<div id="-h">`-h`</div><div id="--help">`--help`</div>|Show the help for the command and parameters<br/>This parameter can also be configured by setting the *_H**HELP* environment variable.| ||
|<div id="--logging.level.com.dqops">`--logging.level.com.dqops`</div>|Default logging level for the DQOps runtime.<br/>This parameter can also be configured by setting the *LOGGING_LEVEL_COM_DQOPS* environment variable.| |*ERROR*<br/>*WARN*<br/>*INFO*<br/>*DEBUG*<br/>*TRACE*<br/>|
|<div id="--logging.level.root">`--logging.level.root`</div>|Default logging level at the root level of the logging hierarchy.<br/>This parameter can also be configured by setting the *LOGGING_LEVEL_ROOT* environment variable.| |*ERROR*<br/>*WARN*<br/>*INFO*<br/>*DEBUG*<br/>*TRACE*<br/>|
|<div id="-of">`-of`</div><div id="--output-format">`--output-format`</div>|Output format for tabular responses<br/>This parameter can also be configured by setting the *_OF**OUTPUT_FORMAT* environment variable.| |*TABLE*<br/>*CSV*<br/>*JSON*<br/>|
|<div id="--server.port">`--server.port`</div>|Sets the web server port to host the DQOps local web UI.<br/>This parameter can also be configured by setting the *SERVER_PORT* environment variable.| ||
|<div id="--silent">`--silent`</div>|Starts DQOps in a silent mode, without showing the banner and any other information.<br/>This parameter can also be configured by setting the *SILENT* environment variable.| ||
|<div id="--spring.config.location">`--spring.config.location`</div>|Sets a path to the folder that has the spring configuration files (application.properties or application.yml) or directly to an application.properties or application.yml file. The format of this value is: --spring.config.location&#x3D;file:./foldername/,file:./alternativeapplication.yml<br/>This parameter can also be configured by setting the *SPRING_CONFIG_LOCATION* environment variable.| ||





