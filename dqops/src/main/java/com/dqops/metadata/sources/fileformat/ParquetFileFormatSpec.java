/*
 * Copyright Â© 2021-Present DQOps, Documati sp. z o.o. (support@dqops.com)
 *
 * This file is licensed under the Business Source License 1.1,
 * which can be found in the root directory of this repository.
 *
 * Change Date: This file will be licensed under the Apache License, Version 2.0,
 * four (4) years from its last modification date.
 */

package com.dqops.metadata.sources.fileformat;

import com.dqops.metadata.basespecs.AbstractSpec;
import com.dqops.metadata.id.ChildHierarchyNodeFieldMap;
import com.dqops.metadata.id.ChildHierarchyNodeFieldMapImpl;
import com.dqops.metadata.id.HierarchyNodeResultVisitor;
import com.dqops.metadata.sources.TableSpec;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonPropertyDescription;
import com.fasterxml.jackson.databind.PropertyNamingStrategies;
import com.fasterxml.jackson.databind.annotation.JsonNaming;
import lombok.EqualsAndHashCode;
import lombok.experimental.FieldNameConstants;

import java.util.List;
import java.util.Objects;

/**
 * Parquet file format specification for querying data in the parquet format files.
 */
@JsonInclude(JsonInclude.Include.NON_NULL)
@JsonNaming(PropertyNamingStrategies.SnakeCaseStrategy.class)
@EqualsAndHashCode(callSuper = true)
@FieldNameConstants
public class ParquetFileFormatSpec extends AbstractSpec {

    private static final ChildHierarchyNodeFieldMapImpl<ParquetFileFormatSpec> FIELDS = new ChildHierarchyNodeFieldMapImpl<>(AbstractSpec.FIELDS) {
        {
        }
    };

    @JsonPropertyDescription("Parquet files generated by legacy writers do not correctly set the UTF8 flag for strings, causing string columns to be loaded as BLOB instead. Set this to true to load binary columns as strings.")
    @JsonInclude(JsonInclude.Include.NON_EMPTY)
    private Boolean binaryAsString = true;

    @JsonPropertyDescription("Whether or not an extra filename column should be included in the result.")
    @JsonInclude(JsonInclude.Include.NON_EMPTY)
    private Boolean filename;

    @JsonPropertyDescription("Whether or not to include the file_row_number column.")
    @JsonInclude(JsonInclude.Include.NON_EMPTY)
    private Boolean fileRowNumber;

    @JsonPropertyDescription("Whether or not to interpret the path as a hive partitioned path.")
    @JsonInclude(JsonInclude.Include.NON_EMPTY)
    private Boolean hivePartitioning;

    @JsonPropertyDescription("Whether the columns of multiple schemas should be unified by name, rather than by position.")
    @JsonInclude(JsonInclude.Include.NON_EMPTY)
    private Boolean unionByName;

    @JsonPropertyDescription("The compression type for the file.")
    @JsonInclude(JsonInclude.Include.NON_EMPTY)
    private CompressionType compression;

    @JsonPropertyDescription("Whether the compression extension is present at the end of the file name.")
    @JsonInclude(JsonInclude.Include.NON_EMPTY)
    private Boolean noCompressionExtension;

    @JsonPropertyDescription("Specifies a custom file name extension. The default is \".parquet\".")
    @JsonInclude(JsonInclude.Include.NON_EMPTY)
    private String fileExtension;

    /**
     * Formats the table options to be used in SQL query. The set (non null) options are added only.
     * @param filePathList The names of files with data.
     * @return The formatted source table with the options.
     */
    public String buildSourceTableOptionsString(List<String> filePathList, TableSpec tableSpec){
        TableOptionsFormatter tableOptionsFormatter = new TableOptionsFormatter("read_parquet", filePathList);
        tableOptionsFormatter.formatValueWhenSet(Fields.binaryAsString, binaryAsString);
        tableOptionsFormatter.formatValueWhenSet(Fields.filename, filename);
        tableOptionsFormatter.formatValueWhenSet(Fields.fileRowNumber, fileRowNumber);
        tableOptionsFormatter.formatValueWhenSet(Fields.hivePartitioning, hivePartitioning);
        tableOptionsFormatter.formatValueWhenSet(Fields.unionByName, unionByName);
        return tableOptionsFormatter.build();
    }

    /**
     * Returns the binary as string option state.
     * @return The binary as string option state.
     */
    public Boolean getBinaryAsString() {
        return binaryAsString;
    }

    /**
     * Sets binary as string option state.
     * @param binaryAsString The binary as string option state.
     */
    public void setBinaryAsString(Boolean binaryAsString) {
        this.binaryAsString = binaryAsString;
    }

    /**
     * Returns whether or not an extra filename column should be included in the result.
     * @return The filename option state.
     */
    public Boolean getFilename() {
        return filename;
    }

    /**
     * Sets that an extra filename column should be included in the result.
     * @param filename The filename option state.
     */
    public void setFilename(Boolean filename) {
        this.filename = filename;
    }

    /**
     * Returns whether or not to include the file_row_number column.
     * @return The file row number option state.
     */
    public Boolean getFileRowNumber() {
        return fileRowNumber;
    }

    /**
     * Sets whether or not to include the file_row_number column.
     * @param fileRowNumber The file row number option state.
     */
    public void setFileRowNumber(Boolean fileRowNumber) {
        this.fileRowNumber = fileRowNumber;
    }

    /**
     * Returns whether or not to interpret the path as a hive partitioned path.
     * @return The hive partitioning option state.
     */
    public Boolean getHivePartitioning() {
        return hivePartitioning;
    }

    /**
     * Sets if interpret the path as a hive partitioned path.
     * @param hivePartitioning Hive partitioning option state.
     */
    public void setHivePartitioning(Boolean hivePartitioning) {
        this.hivePartitioning = hivePartitioning;
    }

    /**
     * Returns whether the columns of multiple schemas should be unified by name, rather than by position.
     * @return The union by nameoption state.
     */
    public Boolean getUnionByName() {
        return unionByName;
    }

    /**
     * Sets whether the columns of multiple schemas should be unified by name, rather than by position.
     * @param unionByName Union by name option state.
     */
    public void setUnionByName(Boolean unionByName) {
        this.unionByName = unionByName;
    }

    /**
     * Returns compression type.
     * @return Compression type.
     */
    public CompressionType getCompression() {
        return compression;
    }

    /**
     * Sets the compression type.
     * @param compression Compression type.
     */
    public void setCompression(CompressionType compression) {
        this.compression = compression;
    }

    /**
     * Returns the accountName
     * @return accountName.
     */
    public Boolean getNoCompressionExtension() {
        return noCompressionExtension;
    }

    /**
     * Sets noCompressionExtension.
     * @param noCompressionExtension noCompressionExtension.
     */
    public void setNoCompressionExtension(Boolean noCompressionExtension) {
        setDirtyIf(!Objects.equals(this.noCompressionExtension, noCompressionExtension));
        this.noCompressionExtension = noCompressionExtension;
    }

    /**
     * Returns a custom file extension.
     *
     * @return Custom file extension
     */
    public String getFileExtension() {
        return fileExtension;
    }

    /**
     * Sets a custom file name extension.
     *
     * @param fileExtension Custom file name extension.
     */
    public void setFileExtension(String fileExtension) {
        setDirtyIf(!Objects.equals(this.fileExtension, fileExtension));
        this.fileExtension = fileExtension;
    }

    @Override
    protected ChildHierarchyNodeFieldMap getChildMap() {
        return FIELDS;
    }

    @Override
    public <P, R> R visit(HierarchyNodeResultVisitor<P, R> visitor, P parameter) {
        return visitor.accept(this, parameter);
    }


    /**
     * Creates and returns a deep clone (copy) of this object.
     */
    @Override
    public ParquetFileFormatSpec deepClone() {
        return (ParquetFileFormatSpec)super.deepClone();
    }

}
